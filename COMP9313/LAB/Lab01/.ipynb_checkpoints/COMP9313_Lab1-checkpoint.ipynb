{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP9313 Lab1 Installation Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In this lab, we will install and configure the programming environment for **MapReduce**<br>\n",
    "For COMP9313-20T2, we will be using the following Python and Java packages (Please note the corresponding versions):<br>\n",
    "    1. Python 3.6.5\n",
    "    2. PySpark 2.4.6\n",
    "    3. Spark 2.4.6\n",
    "    4. Hadoop 2.7\n",
    "    5. Jdk 1.8\n",
    "    \n",
    "The students willing to setup the programming environment in their Personal Computers/ Laptops, should:\n",
    "\n",
    "1. **MAKE SURE THAT YOU INSTALL THE APPROPRIATE VERSIONS (mentioned above).**<br>\n",
    "*  **Your working directory should not contain `SPACE`, otherwise, PySpark may not function appropriately.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install JDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the following link to install jdk 1.8:<br>\n",
    "https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html<br>\n",
    "2. You need to set the environment variable `JAVA_HOME` as follows:\n",
    " -  Windows users: \n",
    "    -  Follow https://javatutorial.net/set-java-home-windows-10, and make sure your JDK directory is correct.\n",
    " - Mac users: \n",
    "    -  Follow https://mkyong.com/java/how-to-set-java_home-environment-variable-on-mac-os-x/, and set the environment variable in `.bash_profile`.\n",
    " - Linux users:\n",
    "    -  Follow https://stackoverflow.com/questions/9612941/how-to-set-java-environment-path-in-ubuntu, and set the environment variable in `~/.bashrc`.\n",
    "\n",
    "\n",
    "Finally, use your terminal/windows shell/linux command line, and type `java -version` to check whether your installation is correct.<br>\n",
    "If you can see **java version 1.8**, it means you have successfully installed the `jdk`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Python (Anaconda) + Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the following link to download the latest version of Anaconda for Python3+.<br>\n",
    "https://www.anaconda.com/products/individual\n",
    " - Mac/Linux Users:\n",
    "    - Follow the link to install Anaonda: https://medium.com/@menuram1126/how-to-install-anaconda-on-ubuntu-16-04-538009ca7936 \n",
    " - Window Users:\n",
    "    - Install via .exe file.\n",
    "    \n",
    "2. Once installed, Anaconda lets you use the `jupyter notebook`. You should be able to run the following command in the terminal to open the Jupyter Notebook.\n",
    " - `jupyter notebook`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Conda Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. After installing the Python (i.e., Anaconda), use the following commands in the terminal to create python programming environment named: `COMP9313`\n",
    "\n",
    "   - `conda create -n COMP9313 python=3.6.5`\n",
    "\n",
    "\n",
    "2. And, activate the environment by using the following command:\n",
    "\n",
    "   - `conda activate COMP9313`\n",
    "   \n",
    "* Note: After activating the environment, you should be able to see `(COMP9313)` in your terminal screen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Install PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Once you have activated your environment `(COMP9313)`, you can use the corresponding `pip` to install the required Python Packages. \n",
    "2. For example, you can use `pip install pyspark==2.4.6` to install the required PySpark Package.\n",
    "\n",
    "\n",
    "Use terminal/windows shell/linux command line, and type `pyspark --version`. You should be able to see: **SPARK version 2.4.6**, which implies you have successfully installed PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Install Spark & Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the following link to download and install Spark and Hadoop<br>\n",
    "https://spark.apache.org/downloads.html \n",
    "\n",
    " - Choose the Spark release: 2.4.6 (June 05 2020)\n",
    " - Choose the package type: Pre-built for Apache Hadoop 2.7\n",
    " - Download Spark: `spark-2.4.6-bin-hadoop2.7.tgz`\n",
    "\n",
    "- You also need to set the environment variable: `export SPARK_HOME=YOUR_SPARK_DIRECTORY_PATH`<br>\n",
    "- Windows users: Download `hadoop winutils` and put it into `YOUR_SPARK_DIRECTORY_PATH/bin`, otherwise, you may not run Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Link Conda Environment with Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Once, we have setup the python environment and installed all the Packages, we need to link the environment `COMP9313` with the `jupyter notebook`. For this, you need to run following commands in the terminal:\n",
    "\n",
    "    - `conda activate COMP9313`\n",
    "    - `conda install -n COMP9313 ipykernel`\n",
    "    - `python -m ipykernel install --user --name COMP9313`\n",
    "\n",
    "\n",
    "2. You should be able to see `COMP9313` in the dropdown menu of Jupyter Notebook: `Kernel` > `Change kernel` > `COMP9313`.\n",
    "\n",
    "3. Select `COMP9313` as your `jupyter notebook` Kernel and run the following script in the notebook to test your environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hello', 3)\n",
      "('world', 2)\n",
      "('word', 1)\n",
      "('count', 1)\n"
     ]
    }
   ],
   "source": [
    "# In this section, we play around a simple Hello-World example.\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    " \n",
    "# create SparkConf and SparkContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"hellow_9313\")\n",
    "sc = SparkContext(conf = conf)\n",
    " \n",
    "# your input data\n",
    "data = [\"hello\", \"world\", \"hello\", \"world\", \"word\", \"count\", \"hello\"]\n",
    "\n",
    "# transform data into spark Rdd\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# use map to form each element x become (x, 1)\n",
    "# then use reduceByKey to aggregate intermediate result from rdd.map()\n",
    "result = rdd.map(lambda word:(word, 1)).reduceByKey(lambda a, b:a + b)\n",
    "\n",
    "# use rdd.collect() to transfrom into collecton and print your final result\n",
    "collection = result.collect()\n",
    "for line in collection:\n",
    "    print(line)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can run this program without any error, you have completed this lab. **CONGRATULATIONS...!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAQs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your program shows: *Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.*, this occurs because you may have multiple versions of python in your machine. For this, you need to set two environment variables:<br>\n",
    "\n",
    "- export PYSPARK_PYTHON = python3\n",
    "- export PYSPARK_DRIVER_PYTHON = python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
