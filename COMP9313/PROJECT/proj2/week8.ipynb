{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"week8\") \n",
    "sc = SparkContext(conf = conf)  # RDD 的入口 entry\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate() # DataFrame 的入口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData = [[\"Chinese Beijing Chinese\", \"c\"],\\\n",
    "                [\"Chinese Chinese Nanjing\", \"c\"],\\\n",
    "                [\"Chinese Macao\", \"c\"],\\\n",
    "                [\"Australia Sydney Chinese\",\"o\"],\\\n",
    "               ]\n",
    "testData = [\"Chinese Chinese Chinese Australia Sydney\"]\n",
    "type(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[66] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainRDD = sc.parallelize(trainingData)\n",
    "testRDD = sc.parallelize(testData)\n",
    "trainRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(category='c', descript='Chinese Beijing Chinese'),\n",
       " Row(category='c', descript='Chinese Chinese Nanjing'),\n",
       " Row(category='c', descript='Chinese Macao'),\n",
       " Row(category='o', descript='Australia Sydney Chinese')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把 RDD 的每一行转化成 Row() 的类型，为 DataFrame 作准备\n",
    "trainRDD = trainRDD.map(lambda e: Row(descript=e[0], category=e[1]))\n",
    "testRDD = testRDD.map(lambda e: Row(descript=e))\n",
    "trainRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+\n",
      "|category|descript                |\n",
      "+--------+------------------------+\n",
      "|c       |Chinese Beijing Chinese |\n",
      "|c       |Chinese Chinese Nanjing |\n",
      "|c       |Chinese Macao           |\n",
      "|o       |Australia Sydney Chinese|\n",
      "+--------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 转化成 DataFrame  \n",
    "trainDF = spark.createDataFrame(trainRDD)\n",
    "testDF = spark.createDataFrame(testRDD)\n",
    "trainDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            descript|\n",
      "+--------------------+\n",
      "|Chinese Beijing C...|\n",
      "|Chinese Chinese N...|\n",
      "|       Chinese Macao|\n",
      "|Australia Sydney ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.select(\"descript\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|category|count|\n",
      "+--------+-----+\n",
      "|       o|    1|\n",
      "|       c|    3|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.groupby(\"category\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+----------------------------+\n",
      "|category|descript                |words                       |\n",
      "+--------+------------------------+----------------------------+\n",
      "|c       |Chinese Beijing Chinese |[chinese, beijing, chinese] |\n",
      "|c       |Chinese Chinese Nanjing |[chinese, chinese, nanjing] |\n",
      "|c       |Chinese Macao           |[chinese, macao]            |\n",
      "|o       |Australia Sydney Chinese|[australia, sydney, chinese]|\n",
      "+--------+------------------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Tokenizer \n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# defined a Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"descript\", outputCol=\"words\")\n",
    "\n",
    "tokenizedDF = tokenizer.transform(trainDF)\n",
    "\n",
    "tokenizedDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------------+\n",
      "|descript                                |words                                         |\n",
      "+----------------------------------------+----------------------------------------------+\n",
      "|Chinese Chinese Chinese Australia Sydney|[chinese, chinese, chinese, australia, sydney]|\n",
      "+----------------------------------------+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testTokenizedDF = tokenizer.transform(testDF)\n",
    "testTokenizedDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+----------------------------+-------------------------+\n",
      "|category|descript                |words                       |features                 |\n",
      "+--------+------------------------+----------------------------+-------------------------+\n",
      "|c       |Chinese Beijing Chinese |[chinese, beijing, chinese] |(6,[0,3],[2.0,1.0])      |\n",
      "|c       |Chinese Chinese Nanjing |[chinese, chinese, nanjing] |(6,[0,2],[2.0,1.0])      |\n",
      "|c       |Chinese Macao           |[chinese, macao]            |(6,[0,1],[1.0,1.0])      |\n",
      "|o       |Australia Sydney Chinese|[australia, sydney, chinese]|(6,[0,4,5],[1.0,1.0,1.0])|\n",
      "+--------+------------------------+----------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer \n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "# defined a CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "# Estimator fit to model\n",
    "cvModel = cv.fit(tokenizedDF)\n",
    "featuredDF = cvModel.transform(tokenizedDF)\n",
    "featuredDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            descript|               words|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Chinese Chinese C...|[chinese, chinese...|(6,[0,4,5],[3.0,1...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testFeaturedDF = cvModel.transform(testTokenizedDF)\n",
    "testFeaturedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+-----+\n",
      "|category|            descript|               words|            features|label|\n",
      "+--------+--------------------+--------------------+--------------------+-----+\n",
      "|       c|Chinese Beijing C...|[chinese, beijing...| (6,[0,3],[2.0,1.0])|  0.0|\n",
      "|       c|Chinese Chinese N...|[chinese, chinese...| (6,[0,2],[2.0,1.0])|  0.0|\n",
      "|       c|       Chinese Macao|    [chinese, macao]| (6,[0,1],[1.0,1.0])|  0.0|\n",
      "|       o|Australia Sydney ...|[australia, sydne...|(6,[0,4,5],[1.0,1...|  1.0|\n",
      "+--------+--------------------+--------------------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# StringIndexer \n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# defined a StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "indexedDF = indexer.fit(featuredDF).transform(featuredDF)\n",
    "\n",
    "indexedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Navie Bayes (调包使用)\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(featuresCol='features', labelCol='label', predictionCol='nb_prediction', smoothing=1.0, modelType='multinomial')\n",
    "nb_model = nb.fit(indexedDF)\n",
    "\n",
    "nb_model.transform(testFeaturedDF).head().nb_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|category|            descript|\n",
      "+--------+--------------------+\n",
      "|       c|Chinese Beijing C...|\n",
      "|       c|Chinese Chinese N...|\n",
      "|       c|       Chinese Macao|\n",
      "|       o|Australia Sydney ...|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用 pipeLine\n",
    "trainDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            descript|\n",
      "+--------------------+\n",
      "|Chinese Chinese C...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "nb_pipeLine = Pipeline(stages=[tokenizer, cv, indexer, nb])\n",
    "\n",
    "pipeModel = nb_pipeLine.fit(trainDF)\n",
    "\n",
    "resDF = pipeModel.transform(testDF)\n",
    "\n",
    "resDF.head().nb_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
